# ğŸ¥ğŸ¿InScene_ver1-LightWeight-FRPipeline-For-In-SceneğŸï¸-Actor-RecognitionğŸ¤”
This repo presents a lightweight face recognition pipeline which can be (in movies and TV shows) in the wild. This work is inspired by the **"X-Ray"** feature on **Amazon Prime video** which gives the viewer - the name and the profile of **all highlighted actors** in a scene when the **pause button** is pressed.

# Demo ğŸ‘‡
<video src="demo.mp4" controls width="640"></video>
[[Link to Demo]](https://youtu.be/8GYcTioPrIA "Click to watch")

# Overview of the pipeline
![Alt text](InScene_Full_Workflow_Github.png)

## ğŸš€ Features

* **MTCNN-based detection**: Accurate multi-scale face detection under varied lighting and pose.
* **Face alignment**: Automatic eye-based alignment for consistent embeddings.
* **Dlib embeddings**: 128D face feature vectors for robust recognition.
* **Actor recognition**: Match faces against a pre-registered set of actors.
* **Video pipeline**: Process frames from camera or video files seamlessly.
* **Modular design**: Components can be swapped (detector, embedder, classifier).

---

## ğŸ“‚ Project Structure

```bash
.
â”œâ”€â”€ data/                 # Store training images & samples per actor
â”œâ”€â”€ models/               # Pretrained or fine-tuned models
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ detector.py       # MTCNN face detector
â”‚   â”œâ”€â”€ aligner.py        # Face alignment utilities
â”‚   â”œâ”€â”€ embedder.py       # Dlib embeddings wrapper
â”‚   â”œâ”€â”€ recognizer.py     # Actor recognition logic
â”‚   â””â”€â”€ pipeline.py       # End-to-end pipeline script
â”œâ”€â”€ notebooks/            # Jupyter notebooks for experiments
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ README.md             # Project documentation (you are here)
â””â”€â”€ LICENSE               # License file
```

---

## ğŸ”§ Installation

1. Clone the repo:

   ```bash
   git clone https://github.com/yourusername/in-scene-actor-recognition.git
   cd in-scene-actor-recognition
   ```

2. Set up environment:

   ```bash
   python3 -m venv venv
   source venv/bin/activate   # (Linux/Mac)
   venv\Scripts\activate      # (Windows)
   ```

3. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```

4. Download Dlib pretrained models (e.g., `shape_predictor_68_face_landmarks.dat`, `dlib_face_recognition_resnet_model_v1.dat`) and place them in the `models/` folder.

---

## ğŸ“˜ Usage

### 1. Register Actors

Provide a folder with subfolders named after each actor:

```bash
data/
â”œâ”€â”€ Actor_1/
â”‚   â”œâ”€â”€ img1.jpg
â”‚   â””â”€â”€ img2.jpg
â”œâ”€â”€ Actor_2/
â”‚   â””â”€â”€ img1.jpg
```

Run the embedding extraction:

```bash
python src/embedder.py --data_dir data/ --save_path models/actors_embeddings.pkl
```

### 2. Run Recognition Pipeline

Process a video file:

```bash
python src/pipeline.py --video input.mp4 --embeddings models/actors_embeddings.pkl
```

Process webcam stream:

```bash
python src/pipeline.py --camera 0 --embeddings models/actors_embeddings.pkl
```

---

## ğŸ§ª Testing

Unit tests are provided for key components. Run them with:

```bash
pytest tests/
```

---

## ğŸ“œ License

This project is licensed under the [MIT License](LICENSE).

---

## ğŸ™Œ Acknowledgements

* [MTCNN](https://kpzhang93.github.io/MTCNN_face_detection_alignment/) for face detection
* [Dlib](http://dlib.net/) for face recognition embeddings
* Open-source community for continuous improvements

---

### â­ If you find this project helpful, donâ€™t forget to star the repo!




